{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7662cbd3-2172-44b3-ac15-5a2e384bc772",
   "metadata": {},
   "source": [
    "## Similarity comparison using GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35a3a5b-4d28-4be1-b98b-5a4ab32b27f9",
   "metadata": {},
   "source": [
    "The model word2vec was developed based on the principle that words appearing in similar contexts tend to have similar meanings, and is a predictive model. However, it only considers a specific window (like +/- 2 words). On the other hand, people proposed that overall co-occurrence of words across an entire corpus could be an awesome pathway to compare similarity as well. That leads to the emergence of count-based model: GloVe. GloVe constructs a global co-occurrence matrix, think of it as a massive table where each cell captures how often two words appear together in the entire text. This sounds similar to some traditional methods in NLP like LSA. However, GloVe turns its concentrate from documents to words, with more stable and robust functions, and better explanability regarding probability. Similar to word2vec, GloVe also turns words into nd vectors. While word2vec is known for its ability to capture analogies and word relationships, GloVe excels in tasks where global understanding of the language is crucial. It balances capturing both local context and global co-occurrence, making it versatile across various NLP tasks. While it’s more memory-intensive because of the massive co-occurrence matrix it builds, it’s highly efficient for learning from global statistics. It’s often pre-trained on datasets like Wikipedia or Common Crawl, so you can leverage those pre-trained vectors for smaller datasets without needing huge computational resources.\n",
    "\n",
    "For tasks like word analogies or semantic similarity, both word2vec and GloVe perform well, though word2vec has a slight edge in capturing complex analogies (king — man + woman = queen). For tasks like sentiment analysis or tasks that benefit from global language understanding, GloVe often excels because it captures both local and global word relationships.\n",
    "\n",
    "word2vec is suitable when considering recommendation system: it is great at understanding relationships between items by treating them like words in a sentence; chatbots: it can help by focusing on word proximity within a dialogue; machine translation: it can capture the local syntax and semantics of words. In short, if the task relies heavily on understanding relationships between words in short sequences, take word2vec. While for GloVe, it is suitable for tasks including text classification: like sentiment analysis, since GloVe captures global co-occurrence statistics, it’s particularly strong in these kinds of tasks where the overall meaning of words across large corpora is more important than just nearby words; similarity scoring: GloVe performs well in tasks that require understanding the broader semantic relationships between words; topic modeling: while trying to group documents or words into clusters based on common themes, GloVe’s global view of word co-occurrence can help create meaningful categories that go beyond local context. In all, if the task requires a more global understanding of language, GloVe is probably the better choice.\n",
    "\n",
    "Gensim provides an efficient implementation of GloVe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38235c34-cd62-4ec0-a4c7-3d88a79399b1",
   "metadata": {},
   "source": [
    "'glove-twitter-25' is trained on Twitter text (2B+ tweets, 1.2m words). Each word is represented as a 25-dimensional vector.\n",
    "\n",
    "It's a skip-gram Word2Vec model, without including a neural network anymore, it only contains the learned vectors. What I have loaded here from Gensim is only the embeddings, not the training weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9061101d-7537-4681-b076-38218a3181c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "glove = \"glove.840B.300d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11d444fc-d650-4e29-95f1-c681fd9a60fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def loadGloveModel(gloveFile):\n",
    "    f = open(gloveFile,'r', encoding='utf8')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split(' ')\n",
    "        word = splitLine[0]\n",
    "        embedding = np.asarray(splitLine[1:], dtype='float32')\n",
    "        model[word] = embedding\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "770099e9-60f6-48a3-9f29-dc3df550537f",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_new = loadGloveModel(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55cd4795-2d33-4cfd-960a-5cc73f4ae0a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Zurich\" in glove_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ae6cfee-01c1-41dc-b25b-a74b39641045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b55f4ac-e8a5-4037-aa5b-f8a2fe00aded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.7667538)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(glove_new['car'], glove_new['vehicle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2666e5d5-35d6-4dfd-ae75-4189c946e4c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.8785519)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(glove_new['lol'], glove_new['hahaha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1a1d245-61b5-4cc5-a765-6b57728e756e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def most_similar(word, top_k=10):\n",
    "    \n",
    "    target_vec = glove_new[word]\n",
    "    \n",
    "    similarities = []\n",
    "    for other_word, other_vec in glove_new.items():\n",
    "        if other_word == word:\n",
    "            continue\n",
    "        sim = cosine_similarity(target_vec, other_vec)\n",
    "        similarities.append((sim, other_word))\n",
    "    \n",
    "    top = heapq.nlargest(top_k, similarities)\n",
    "    return [(w, float(s)) for s, w in top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d78abd51-e3e8-4b19-9c40-c269c7302ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bananas', 0.8091707229614258),\n",
       " ('pineapple', 0.7421034574508667),\n",
       " ('coconut', 0.7215185761451721),\n",
       " ('strawberry', 0.7120676040649414),\n",
       " ('mango', 0.699190080165863),\n",
       " ('carrot', 0.6791642308235168),\n",
       " ('fruit', 0.6714836359024048),\n",
       " ('pumpkin', 0.6642457246780396),\n",
       " ('peanut', 0.6637847423553467),\n",
       " ('blueberry', 0.6517722010612488)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"banana\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d329cf70-3a93-435c-a77a-46c52d58881d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.78808445)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(glove_new['king'] - glove_new['man'] + glove_new['woman'], glove_new['queen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ae307e3-5966-4a16-9beb-e2f641cf8dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.42697847)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(glove_new['vienna'], glove_new['alps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ebda8ff-8514-40dc-8a6d-dff4cade9de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Alps', 0.7212010622024536),\n",
       " ('alpine', 0.5882586240768433),\n",
       " ('alp', 0.586816132068634),\n",
       " ('chamonix', 0.573789656162262),\n",
       " ('alpes', 0.5729764699935913),\n",
       " ('switzerland', 0.5561335682868958),\n",
       " ('austria', 0.5324950218200684),\n",
       " ('bavarian', 0.5103736519813538),\n",
       " ('dolomites', 0.5045719742774963),\n",
       " ('zermatt', 0.502839207649231)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"alps\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "378d55aa-d99b-4da6-8a0d-fee06e37d3ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.157181)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(glove_new['st.'], glove_new['alps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc91cc5a-7540-4d17-9140-b4cd8d063f96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.26138788)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(glove_new['wolfgang'], glove_new['alps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96af076b-63b3-484a-82a0-8986bc7cdd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('glove_new', glove_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bcf813-ff61-4e4d-835e-b248338a628c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myGeoKG",
   "language": "python",
   "name": "mygeokg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
